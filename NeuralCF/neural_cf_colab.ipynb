{
 "cells": [
  {
   "cell_type": "code",
   "id": "c1a584edae555d71",
   "metadata": {
    "id": "c1a584edae555d71",
    "ExecuteTime": {
     "end_time": "2024-12-05T11:34:37.152910Z",
     "start_time": "2024-12-05T11:34:37.140955Z"
    }
   },
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "google_colab = False\n",
    "with_validation = True\n",
    "latent_dim_gmf = 30\n",
    "latent_dim_mlp = 3\n",
    "batch_size = 256\n",
    "epochs = 30\n",
    "LR = 0.001\n",
    "WEIGHT_DECAY = 1e-4"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T11:34:37.230500Z",
     "start_time": "2024-12-05T11:34:37.219932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if google_colab:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive')\n",
    "    train_path = \"/content/drive/MyDrive/DIS_recomander_sys/train.csv\"\n",
    "    test_path = \"/content/drive/MyDrive/DIS_recomander_sys/test.csv\"\n",
    "else:\n",
    "    train_path = \"../data/train.csv\"\n",
    "    test_path = \"../data/test.csv\""
   ],
   "id": "b191c2ca012d5548",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "id": "4445f6c38d4e50a5",
    "ExecuteTime": {
     "end_time": "2024-12-05T11:34:37.252573Z",
     "start_time": "2024-12-05T11:34:37.242195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a dataset class\n",
    "class InteractionDataset(Dataset):\n",
    "    def __init__(self, user_item_pairs, ratings):\n",
    "        self.user_item_pairs = user_item_pairs\n",
    "        self.ratings = ratings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_item_pairs[idx], self.ratings[idx]\n",
    "\n",
    "\n",
    "# Define the NCF model\n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, latent_dim_gmf, latent_dim_mlp):\n",
    "        super(NCF, self).__init__()\n",
    "        # Embeddings for MLP part\n",
    "        self.user_embedding_mlp = nn.Embedding(num_users, latent_dim_mlp)\n",
    "        self.item_embedding_mlp = nn.Embedding(num_items, latent_dim_mlp)\n",
    "\n",
    "        # Embeddings for GMF part\n",
    "        self.user_embedding_gmf = nn.Embedding(num_users, latent_dim_gmf)\n",
    "        self.item_embedding_gmf = nn.Embedding(num_items, latent_dim_gmf)\n",
    "\n",
    "        # Fully connected layers for MLP\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(latent_dim_mlp * 2, 128),\n",
    "            nn.BatchNorm1d(128),  # Batch normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 80),\n",
    "            nn.BatchNorm1d(80),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        # Final output layer (after concatenating GMF and MLP outputs)\n",
    "        self.final_layer = nn.Linear(latent_dim_gmf + 80, 1)  # GMF (latent_dim) + MLP output (64)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        # MLP embeddings\n",
    "        user_embed_mlp = self.user_embedding_mlp(user)  # Shape: [batch_size, latent_dim]\n",
    "        item_embed_mlp = self.item_embedding_mlp(item)  # Shape: [batch_size, latent_dim]\n",
    "\n",
    "        # GMF embeddings\n",
    "        user_embed_gmf = self.user_embedding_gmf(user)  # Shape: [batch_size, latent_dim]\n",
    "        item_embed_gmf = self.item_embedding_gmf(item)  # Shape: [batch_size, latent_dim]\n",
    "\n",
    "        # GMF interaction (element-wise product)\n",
    "        gmf_output = torch.mul(user_embed_gmf, item_embed_gmf)  # Shape: [batch_size, latent_dim]\n",
    "\n",
    "        # MLP interaction (concatenation)\n",
    "        mlp_input = torch.cat([user_embed_mlp, item_embed_mlp], dim=-1)  # Shape: [batch_size, latent_dim * 2]\n",
    "        mlp_output = self.fc_layers(mlp_input)  # Shape: [batch_size, 64]\n",
    "\n",
    "        # Concatenate GMF and MLP outputs\n",
    "        combined = torch.cat([gmf_output, mlp_output], dim=-1)  # Shape: [batch_size, latent_dim + 64]\n",
    "\n",
    "        # Final prediction layer\n",
    "        output = self.final_layer(combined).squeeze()  # Shape: [batch_size]\n",
    "\n",
    "        return output\n"
   ],
   "id": "4445f6c38d4e50a5",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "id": "b5ffda6a194f9fc",
    "ExecuteTime": {
     "end_time": "2024-12-05T11:34:37.487846Z",
     "start_time": "2024-12-05T11:34:37.301646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare data\n",
    "train_df = pd.read_csv(train_path)\n",
    "user_item_pairs = train_df[[\"user_id\", \"book_id\"]].values\n",
    "#Map the user_id and book_id to a unique index\n",
    "user_to_index = {user_id: idx for idx, user_id in enumerate(train_df['user_id'].unique())}\n",
    "item_to_index = {book_id: idx for idx, book_id in enumerate(train_df['book_id'].unique())}\n",
    "train_df['user_idx'] = train_df['user_id'].apply(lambda x: user_to_index[x])\n",
    "train_df['book_idx'] = train_df['book_id'].apply(lambda x: item_to_index[x])\n",
    "user_item_index_pairs = train_df[[\"user_idx\", \"book_idx\"]].values\n",
    "ratings = train_df[\"rating\"].values\n",
    "\n",
    "# Define constants\n",
    "num_users = len(user_to_index)\n",
    "num_items = len(item_to_index)\n",
    "print(num_users, num_items)\n",
    "\n",
    "# DataLoader\n",
    "dataset = InteractionDataset(user_item_index_pairs, ratings)\n",
    "\n",
    "# Split into training and validation sets (80-20 split)\n",
    "if with_validation:\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "else:\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = None\n",
    "\n",
    "# Train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NCF(num_users, num_items, latent_dim_gmf, latent_dim_mlp).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)"
   ],
   "id": "b5ffda6a194f9fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18905 15712\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "if google_colab:\n",
    "    print(torch.cuda.is_available())\n",
    "    print(torch.cuda.memory_allocated())\n",
    "    print(torch.cuda.memory_reserved())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qfk7M7ANOBHM",
    "outputId": "5058cb18-8f4c-43e9-e2ff-57b118375d00",
    "ExecuteTime": {
     "end_time": "2024-12-05T11:34:37.544961Z",
     "start_time": "2024-12-05T11:34:37.537146Z"
    }
   },
   "id": "Qfk7M7ANOBHM",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50b71f7115902e73",
    "outputId": "490c1637-86b5-4131-9a94-c1f68b74021d",
    "ExecuteTime": {
     "end_time": "2024-12-05T11:36:11.901827Z",
     "start_time": "2024-12-05T11:34:37.755174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in tqdm(range(epochs), desc=\"Epochs\", unit=\"epoch\"):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Training loop\n",
    "    for user_item, rating in train_loader:\n",
    "        user, item = user_item[:, 0].long().to(device), user_item[:, 1].long().to(device)\n",
    "        rating = rating.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(user, item)\n",
    "        loss = criterion(predictions, rating)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # Validation loop\n",
    "    if with_validation:\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for user_item, rating in val_loader:\n",
    "                user, item = user_item[:, 0].long().to(device), user_item[:, 1].long().to(device)\n",
    "                rating = rating.float().to(device)\n",
    "                predictions = model(user, item)\n",
    "                loss = criterion(predictions, rating)\n",
    "                total_val_loss += loss.item()\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}, Train Loss: {total_train_loss / len(train_loader):.4f}, Val Loss: {total_val_loss / len(val_loader):.4f}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {total_train_loss / len(train_loader):.4f}\")\n"
   ],
   "id": "50b71f7115902e73",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   3%|▎         | 1/30 [00:03<01:44,  3.59s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 2.0643, Val Loss: 1.4776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   7%|▋         | 2/30 [00:06<01:31,  3.27s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 1.4483, Val Loss: 1.3356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|█         | 3/30 [00:09<01:26,  3.21s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 1.3876, Val Loss: 1.2439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  13%|█▎        | 4/30 [00:12<01:20,  3.11s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 1.3401, Val Loss: 1.1455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  17%|█▋        | 5/30 [00:15<01:18,  3.15s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 1.2907, Val Loss: 1.0695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██        | 6/30 [00:18<01:13,  3.08s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 1.2416, Val Loss: 1.0086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  23%|██▎       | 7/30 [00:22<01:14,  3.25s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 1.1885, Val Loss: 0.9468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  27%|██▋       | 8/30 [00:25<01:12,  3.30s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 1.1266, Val Loss: 0.8966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  30%|███       | 9/30 [00:29<01:11,  3.38s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 1.0699, Val Loss: 0.8575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  33%|███▎      | 10/30 [00:32<01:07,  3.37s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 1.0098, Val Loss: 0.8308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  37%|███▋      | 11/30 [00:36<01:04,  3.38s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Train Loss: 0.9537, Val Loss: 0.7995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  40%|████      | 12/30 [00:39<00:59,  3.31s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Train Loss: 0.9068, Val Loss: 0.7801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  43%|████▎     | 13/30 [00:42<00:54,  3.20s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Train Loss: 0.8579, Val Loss: 0.7606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  47%|████▋     | 14/30 [00:45<00:49,  3.11s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Train Loss: 0.8191, Val Loss: 0.7486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  50%|█████     | 15/30 [00:48<00:45,  3.04s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Train Loss: 0.7798, Val Loss: 0.7376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  53%|█████▎    | 16/30 [00:50<00:42,  3.00s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Train Loss: 0.7454, Val Loss: 0.7265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  57%|█████▋    | 17/30 [00:53<00:38,  2.99s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Train Loss: 0.7138, Val Loss: 0.7229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  60%|██████    | 18/30 [00:57<00:37,  3.14s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Train Loss: 0.6774, Val Loss: 0.7238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  63%|██████▎   | 19/30 [01:00<00:34,  3.14s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Train Loss: 0.6570, Val Loss: 0.7118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  67%|██████▋   | 20/30 [01:03<00:31,  3.10s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Train Loss: 0.6222, Val Loss: 0.7111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  70%|███████   | 21/30 [01:06<00:27,  3.07s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Train Loss: 0.6016, Val Loss: 0.7273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  73%|███████▎  | 22/30 [01:09<00:24,  3.11s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Train Loss: 0.5806, Val Loss: 0.7111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  77%|███████▋  | 23/30 [01:12<00:21,  3.05s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Train Loss: 0.5611, Val Loss: 0.7303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  80%|████████  | 24/30 [01:15<00:18,  3.01s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Train Loss: 0.5420, Val Loss: 0.7324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  83%|████████▎ | 25/30 [01:18<00:14,  3.00s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Train Loss: 0.5268, Val Loss: 0.7347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  87%|████████▋ | 26/30 [01:21<00:12,  3.02s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Train Loss: 0.5123, Val Loss: 0.7487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  90%|█████████ | 27/30 [01:24<00:09,  3.05s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Train Loss: 0.4961, Val Loss: 0.7529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  93%|█████████▎| 28/30 [01:27<00:06,  3.07s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Train Loss: 0.4774, Val Loss: 0.7521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  97%|█████████▋| 29/30 [01:30<00:03,  3.07s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Train Loss: 0.4657, Val Loss: 0.7486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 30/30 [01:34<00:00,  3.14s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Train Loss: 0.4540, Val Loss: 0.7441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "ExecuteTime": {
     "end_time": "2024-12-05T11:36:17.726344Z",
     "start_time": "2024-12-05T11:36:11.979215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "\n",
    "# Imputation (predict missing values)\n",
    "def predict_missing_values(model, test_df, user_to_index, item_to_index):\n",
    "    test_df['user_idx'] = test_df['user_id'].apply(lambda x: user_to_index[x])\n",
    "    test_df['book_idx'] = test_df['book_id'].apply(lambda x: item_to_index[x])\n",
    "    user_item_pairs = test_df[['user_idx', 'book_idx']].values\n",
    "\n",
    "    model.eval()\n",
    "    submission = []\n",
    "    for user, item in user_item_pairs:\n",
    "        with torch.no_grad():\n",
    "            user_tensor = torch.tensor([user]).to(device)\n",
    "            item_tensor = torch.tensor([item]).to(device)\n",
    "            prediction = model(user_tensor, item_tensor).item()\n",
    "        submission.append([prediction])\n",
    "    return submission\n",
    "\n",
    "\n",
    "submission = predict_missing_values(model, test_df, user_to_index, item_to_index)\n",
    "#save the submission\n",
    "submission_df = pd.DataFrame(submission, columns=['rating'])\n",
    "submission_df.to_csv(\"submission.csv\", index=True)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "id": "6a6549cc0b0701ef"
   },
   "cell_type": "markdown",
   "source": [
    "### Kaggle results"
   ],
   "id": "6a6549cc0b0701ef"
  },
  {
   "metadata": {
    "id": "1d19b23f0bd11e2d"
   },
   "cell_type": "markdown",
   "source": [
    "time : 3min\n",
    "\n",
    "score : 0.88"
   ],
   "id": "1d19b23f0bd11e2d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
